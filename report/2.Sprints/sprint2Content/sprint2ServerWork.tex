\section{Server}\label{SEC:S2ServerWork}
During "Sprint 2" the project group was tasked with ordering new servers from ITS, as well as investigating how the old servers were configured. 
In the following sections we will describe our work done on these tasks, and show how new issues emerged afterwards. 

\subsection{Server requisition}
During "Sprint 1" the server meta-group had established how the old server architecture was build. 
Based on those findings, the server group made a suggestion for a new server layout, and it was simultaneously deemed easier to request some new servers and migrate the existing data from the old servers to the new ones, rather than to clean up the original mess. 

We made a request to ITS at AAU for new servers with the promise of shutting down the old servers once the new system is up and running. The original server layout was as following:

\begin{table}[H]

\begin{tabular}{|l|l|l|l|l|l|}
\hline
Hostname & IP 			& RAM 	& CPU 		& Disk 	& OS 					\\ \hline
Master00 & 192.38.56.37 	& 2 GB 	& 2 Cores 	& 22 GB 	& CentOS Linux 7.5.1804 	\\ \hline
Node01 	& 172.19.0.244 	& 2 GB 	& 1 Core 	& 22 GB 	& CentOS Linux 7.5.1804 	\\ \hline
Node02	& 172.19.0.245	& 2 GB	& 1 Core		& 22 GB	& CentOS Linux 7.5.1804	\\ \hline
Node03	& 192.38.56.36	& 2 GB	& 1 Core 	& 22 GB	& CentOS Linux 7.5.1804 	\\ \hline
GitLab 	& 192.38.56.136	& 4 GB	& 2 Cores 	& 46 GB 	& CentOS Linux 7.5.1804 	\\ \hline
web01	& 192.38.56.38	& 2 GB	& 1 Core		& 22 GB 	& CentOS Linux 7.4.1708 	\\ \hline
Backup01	& 172.19.0.235	& 4 GB	& 2 Cores	& 200 GB & CentOS Linux 7.2.1511 	\\ \hline
\end{tabular}
\end{table}

To replace this setup, the following new setup was requested and granted by ITS:

\begin{table}[H]
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Hostname & IP				& RAM 	& CPU 		& Disk 	& OS 					\\ \hline
Master00 & 172.19.10.29	& 4 GB	& 2 Cores	& 24 GB	& Ubuntu Server 18.04.2	\\ \hline
Master01 & 172.19.10.30	& 4 GB	& 2 Cores	& 24 GB	& Ubuntu Server 18.04.2	\\ \hline			
Node00 	& 172.19.10.31	& 2 GB	& 1 Core		& 24 GB	& Ubuntu Server 18.04.2	\\ \hline
Node01 	& 172.19.10.32	& 2 GB	& 1 Core		& 24 GB	& Ubuntu Server 18.04.2	\\ \hline
Node02 	& 172.19.10.33	& 2 GB	& 1 Core		& 24 GB	& Ubuntu Server 18.04.2	\\ \hline
Node03 	& 172.19.10.34	& 2 GB	& 1 Core		& 24 GB	& Ubuntu Server 18.04.2	\\ \hline
\end{tabular}
\end{table}

One of the problems experienced by the server meta-group was the constant pain of organizing the user account and passwords for accessing and managing all the servers. 
In order to mitigate these problems ITS suggested that the servers could be setup using Microsoft Active Directory(AD) authentication, i.e. each of the server group members can sign in to the servers using their student email and password as credentials. 
This makes it easier for the following semesters to take over the project since they could simply be added to the AD user group by ITS. 

With access to the servers' operating systems, a software reboot can be attempted, but, since some of the servers may hang during the boot process, some of the server group members were given access to the VMware Vsphere back-end, as can be seen on \autoref{FIG:VMwareVSphereClient}. 
Through this system the power settings for the virtual servers can be accessed and controlled directly by the students. 
\figur{1}{images/VMwareVSphereClient.png}{Screenshot of the dashboard from VMware VSphere Client.}{FIG:VMwareVSphereClient}

Another problem with the old servers was that all servers had a public IP address where all ports are exposed to the internet. 
As a result of this, each server has a large number of failed login attempts.
On \autoref{FIG:OlderServerPortScan} the output of a port scan of the older servers can be seen. 
With the new server architecture a similar scan should be minimal. 
\figur{0.6}{images/GirafOldServerPortScan.png}{Output of nmap scan on the older server infrastructure.}{FIG:OlderServerPortScan}

In order to avoid this in the future, and to follow best practices, only the two servers acting as master servers are given a public IP address, and to limit access even more only port $80$ and $443$ are open on the two servers. 
This will limit the attack surface open to attackers and to normal users. 
If a an administrator needs to access one of the servers, the admin must be located on the AAU network, which can be accomplished by either being on AAU's premises, or via a Virtual Private Network(VPN).

\subsection{New server setup}

Once the new servers were setup by ITS, and we were given access to them, all servers were setup as a Docker Swarm. 
On each server, Docker Community Edition was installed, and the master servers (Master00 and Master01) have been added as Swarm Managers whereas the node servers (Node00, Node01, Node02 and Node03) have been added as workers. 

Once each server has been added to the Docker Swarm, the command \lstinline[columns=fixed]{docker node ls} can be run to check the status of the Docker Swarm.
The output can be seen on \autoref{FIG:DockerNodeLS}.
\figur{1}{images/DockerNodeLS.png}{Output of docker node ls showing the nodes of the Docker Swarm.}{FIG:DockerNodeLS}

With Docker installed and with the Docker Swarm setup, the entire hosting envionment is ready for the migration from the old server architecture to the new.

\subsection{Status quo}
At the end of this sprint the new servers are ready for deploying the Giraf infrastructure. 
However an early migration from the old server to the new has resulted in a working state that does not follow best practices and are not sustainable for long term deployment. 

The \textbf{API} is not well intended to being migrated to Docker Swarm since it is dependant on the local appsettings file which contains the username and password for the database in plaintext. 

The \textbf{Database} does naturally move into a Docker Swarm environment but does not naturally scale across a multi node server infrastructure do to some file sharing problems. 

One huge problem with the old setup where that every thing is open to the public internet and does thus make it an huge target for hackers. 
On the new setup this has been mitigated by using a reverse proxy server that will pass the traffic intended for the different services. 
This proxy setup has been implemented in a hurry and should be updated later as the rest of the services are added to the Docker Swarm. 

\subsection{Open issues for future sprints}
After the initial setup of the servers, several new issues were created on the project GitHub. 
Each issue is a task needed to be done as a part of the project, and is as follows:

\begin{itemize}
\item Setup NGINX as a reverse proxy for the Docker Swarm and document the work(\href{https://github.com/aau-giraf/wiki/issues/31}{Issue \texttt{\#}31}).
\item Migrate the WEB-API to Docker and document the work(\href{https://github.com/aau-giraf/wiki/issues/34}{Issue \texttt{\#}34}).
\item Migrate the MySQL Database to Docker and document the work(\href{https://github.com/aau-giraf/wiki/issues/35}{Issue \texttt{\#}35}).
\end{itemize}

The task of deciding whether the issues should be included in the next sprint is handed over to the product owner group.
